{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PArtD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpoTvrzquZXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "443d670d-37d1-428e-9b27-205f48578917"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "data=[]\n",
        "testa=[]\n",
        "\n",
        "#Upload the training data\n",
        "with open('olid-training-v1.0.tsv') as tsvfile:\n",
        "  reader = csv.reader(tsvfile, delimiter='\\t')\n",
        "  for line in reader:\n",
        "    ids = line[0]\n",
        "    tweet = line[1]\n",
        "    subtask_a = line[2]\n",
        "    subtask_b = line[3]\n",
        "    subtask_c = line[4]\n",
        "    data.append((ids,tweet,subtask_a,subtask_b,subtask_c))\n",
        "\n",
        "df = pd.DataFrame(data[1:],columns=['id','tweet','subtask_a','subtask_b','subtask_c'])\n",
        "\n",
        "#upload the test data\n",
        "with open('testset-levela.tsv') as tsvfilea:\n",
        "  readera = csv.reader(tsvfilea, delimiter='\\t')\n",
        "  for line in readera:\n",
        "    ids = line[0]\n",
        "    tweet = line[1]\n",
        "    testa.append((ids,tweet))\n",
        "\n",
        "dftesta = pd.DataFrame(testa[1:],columns=['id','tweet'])\n",
        "\n",
        "#store the training labels to a list\n",
        "all_labels=[]\n",
        "\n",
        "all_labels=pd.concat([df['subtask_a'],df['subtask_b'],df['subtask_c']], axis=1)\n",
        "all_labels=all_labels.values.tolist()\n",
        "\n",
        "\n",
        "#upload test labels and store them to a list\n",
        "testlabels=[]\n",
        "\n",
        "dflaba = pd.read_csv('labels-levela.csv', names=['ids', 'label'])\n",
        "dflabb = pd.read_csv('labels-levelb.csv', names=['ids', 'label'])\n",
        "dflabc = pd.read_csv('labels-levelc.csv', names=['ids', 'label'])\n",
        "\n",
        "test_labels=pd.concat([dflaba['label'],dflabb['label'],dflabc['label']] ,axis=1)\n",
        "test_labels['label'] = test_labels['label'].astype(str)\n",
        "test_labels=test_labels.values.tolist()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNs1ACZxhL42",
        "colab_type": "code",
        "outputId": "0ba0d49d-b51f-4ee7-f16a-bada60945c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Preprocessing function\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import string\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "def preprocessing(text):\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    #print(tokens)\n",
        "\n",
        "\n",
        "   #remove punctuation\n",
        "    #pattern = '[0-9]'\n",
        "    # tokens = [re.sub(r\"[^a-zA-Z0-9]\", '', text) for t in text]\n",
        "    #tokens = [re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1',t) for t in text]\n",
        "   # tokens = [re.sub(pattern, '', tokens) for t in tokens]\n",
        "\n",
        " \n",
        "    # lowercasing\n",
        "    tokens = [t.lower() for t in tokens]\n",
        "\n",
        "    # stopword removal- benefits are it removes rare words\n",
        "    stop = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop]\n",
        "    #print(tokens)\n",
        "\n",
        "  \n",
        "    return tokens"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5gkcPIL2-4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocess the training data and store them to a list\n",
        "for index, line in df.iterrows():\n",
        "  #print(preprocessing(line.tweet))\n",
        "  df['tweet'][index] = preprocessing(str(line.tweet))\n",
        "\n",
        "normalized=[]\n",
        "\n",
        "for index, line in df.iterrows():\n",
        "  normalized.append(df['tweet'][index])\n",
        "\n",
        "#Preprocess the test data and store them to a list\n",
        "for index, line in dftesta.iterrows():\n",
        "  #print(preprocessing(line.tweet))\n",
        "  dftesta['tweet'][index] = preprocessing(str(line.tweet))\n",
        "\n",
        "normalized_testa=[]\n",
        "\n",
        "for index, line in dftesta.iterrows():\n",
        "  normalized_testa.append(dftesta['tweet'][index])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG1ggcM0ks7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2idx,idx2word,sents_as_ids for training\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(normalized)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "word2idx[\"<PAD>\"] = 0\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "\n",
        "sents_as_ids=[]\n",
        "for line in range(len(normalized)):\n",
        "    wids=[]\n",
        "    for a in normalized[line]:\n",
        "      wids.append(word2idx[a])\n",
        "    sents_as_ids.append(wids)\n",
        "\n",
        "#word2idx,idx2word,sents_as_ids for testing\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(normalized_testa)\n",
        "\n",
        "word2idxa = tokenizer.word_index\n",
        "word2idxa[\"<PAD>\"] = 0\n",
        "idx2worda = {v:k for k, v in word2idxa.items()}\n",
        "\n",
        "sents_as_idsa=[]\n",
        "for line in range(len(normalized_testa)):\n",
        "    widsa=[]\n",
        "    for a in normalized_testa[line]:\n",
        "      widsa.append(word2idxa[a])\n",
        "    sents_as_idsa.append(widsa)\n",
        "\n",
        "vocab_size = len(word2idx)+1\n",
        "embed_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bal6t4ecmaTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9359fb94-a4b7-4145-b72e-11ecac22c1bc"
      },
      "source": [
        "print('Number of unique words:', len(idx2word))\n",
        "print('\\nVocabulary Sample:', list(idx2word.items())[:10])\n",
        "print('\\nSample word2idx: ', list(word2idx.items())[:10])\n",
        "print('\\nAbove sentence as a list of ids:' , sents_as_ids[:3])\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words: 18974\n",
            "\n",
            "Vocabulary Sample: [(1, 'user'), (2, 'url'), (3, 'liberals'), (4, 'gun'), (5, 'control'), (6, 'antifa'), (7, 'like'), (8, 'maga'), (9, 'conservatives'), (10, 'people')]\n",
            "\n",
            "Sample word2idx:  [('user', 1), ('url', 2), ('liberals', 3), ('gun', 4), ('control', 5), ('antifa', 6), ('like', 7), ('maga', 8), ('conservatives', 9), ('people', 10)]\n",
            "\n",
            "Above sentence as a list of ids: [[1, 256, 2498, 184, 45], [1, 1, 22, 205, 834, 1, 8, 772, 2], [3486, 6220, 1732, 2285, 1643, 4913, 1305, 1733, 48, 8875, 189, 3487, 3488, 4914, 2, 3486, 8, 272, 721, 395]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW8coPbUqTSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TASKA binary classification - encoding the labels\n",
        "\n",
        "#encode the label\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "Encoder = LabelEncoder()\n",
        "\n",
        "#encode the labels for TaskA\n",
        "binarylabels = Encoder.fit_transform(df['subtask_a'])\n",
        "testbinarylabels = Encoder.fit_transform(dflaba['label'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuYz7mn_ZL9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TASKA,B,C MULTI classification - encoding the labels\n",
        "\n",
        "#word2idx,idx2word,sents_as_ids for training labels, we need to translate the labels to numbers for the model\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(all_labels)\n",
        "\n",
        "word2idxm = tokenizer.word_index\n",
        "word2idxm[\"<PAD>\"] = 0\n",
        "idx2wordm = {v:k for k, v in word2idxm.items()}\n",
        "\n",
        "sents_as_idsm=[]\n",
        "for line in range(len(all_labels)):\n",
        "    widsm=[]\n",
        "    for a in all_labels[line]:\n",
        "        for w in text.text_to_word_sequence(a):\n",
        "          widsm.append(word2idxm[w])\n",
        "    sents_as_idsm.append(widsm)\n",
        "\n",
        "#word2idx,idx2word,sents_as_ids for testing labels, we need to translate the labels to numbers for the model\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(test_labels)\n",
        "\n",
        "word2idxt = tokenizer.word_index\n",
        "word2idxt[\"<PAD>\"] = 0\n",
        "idx2wordt = {v:k for k, v in word2idxt.items()}\n",
        "\n",
        "sents_as_idst=[]\n",
        "for line in range(len(test_labels)):\n",
        "    widst=[]\n",
        "    for a in test_labels[line]:\n",
        "        for w in text.text_to_word_sequence(a):\n",
        "          widst.append(word2idxt[w])\n",
        "    sents_as_idst.append(widst)\n",
        "\n",
        "multilabels=np.array(sents_as_idsm)\n",
        "testmultilabels=np.array(sents_as_idst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p_whdyhzUjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preparing input data\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAXIMUM_LENGTH = 256\n",
        "\n",
        "\n",
        "preprocessed_train_data = keras.preprocessing.sequence.pad_sequences(sents_as_ids,\n",
        "                                                        value=word2idx[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAXIMUM_LENGTH)\n",
        "\n",
        "\n",
        "preprocessed_test_data = keras.preprocessing.sequence.pad_sequences(sents_as_idsa,\n",
        "                                                        value=word2idxa[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAXIMUM_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMRJPfZgBGbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "821eebf2-69b6-4a6c-8178-d5420cb9f580"
      },
      "source": [
        "print(preprocessed_test_data[1])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 608 1728    3 1729  336  461   23   16   97    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thua6w3VzjDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM model for binary classification\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.layers import Embedding,LSTM\n",
        "\n",
        "EMBED_SIZE = 100\n",
        "\n",
        "MAXIMUM_LENGTH = 256\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
        "model.add(LSTM(units=100, activation='tanh'))   #change activation function to tanh\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cotmwwwh1Ivk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "e33eca53-7f60-467e-f28e-f79f7ac47205"
      },
      "source": [
        " print(model.summary())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 256, 100)          1897500   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,978,001\n",
            "Trainable params: 1,978,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTQkMJJkS7Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LSTM model for multi classification\n",
        "\n",
        "model2 = models.Sequential()\n",
        "model2.add(Embedding(input_dim=vocab_size,output_dim=EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
        "model2.add(LSTM(units=100, activation='tanh'))   #change activation function to tanh\n",
        "model2.add(layers.Dense(3, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcwOKJ7mjhWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "bf4569c1-372c-4e4a-90e9-beb466c7a70e"
      },
      "source": [
        " print(model2.summary())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 256, 100)          1897500   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 1,978,203\n",
            "Trainable params: 1,978,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXRSyWZB1Wdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "599fcd51-2f5f-4146-bd64-e9236692555a"
      },
      "source": [
        "#When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set\n",
        "\n",
        "X_val = np.array(preprocessed_train_data[:2000])\n",
        "partial_X_train = np.array(preprocessed_train_data[2000:])\n",
        "\n",
        "y_val = np.array(binarylabels[:2000])\n",
        "partial_y_train = np.array(binarylabels[2000:])\n",
        "\n",
        "y_val_m = np.array(multilabels[:2000])\n",
        "partial_y_train_m = np.array(multilabels[2000:])\n",
        "\n",
        "print(X_val)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    1   256  2498 ...     0     0     0]\n",
            " [    1     1    22 ...     0     0     0]\n",
            " [ 3486  6220  1732 ...     0     0     0]\n",
            " ...\n",
            " [    1     1 10409 ...     0     0     0]\n",
            " [    1   831   122 ...     0     0     0]\n",
            " [    1  1863     0 ...     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TX-aVY_116a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "9cdf93dc-7535-4a7e-e6a8-9ce1c9322fea"
      },
      "source": [
        "#Then we start to train the model for 3 epochs in mini-batches of 100 samples and monitor the model's loss and accuracy on the validation set.\n",
        "\n",
        "\n",
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=3,\n",
        "                    batch_size=100,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "\n",
        "history = model2.fit(partial_X_train,\n",
        "                    partial_y_train_m,\n",
        "                    epochs=3,\n",
        "                    batch_size=100,\n",
        "                    validation_data=(X_val, y_val_m),\n",
        "                    verbose=1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11240 samples, validate on 2000 samples\n",
            "Epoch 1/3\n",
            "11240/11240 [==============================] - 72s 6ms/step - loss: 0.6395 - acc: 0.6653 - val_loss: 0.6395 - val_acc: 0.6630\n",
            "Epoch 2/3\n",
            "11240/11240 [==============================] - 71s 6ms/step - loss: 0.6357 - acc: 0.6685 - val_loss: 0.6392 - val_acc: 0.6630\n",
            "Epoch 3/3\n",
            "11240/11240 [==============================] - 71s 6ms/step - loss: 0.6360 - acc: 0.6685 - val_loss: 0.6391 - val_acc: 0.6630\n",
            "Train on 11240 samples, validate on 2000 samples\n",
            "Epoch 1/3\n",
            "11240/11240 [==============================] - 72s 6ms/step - loss: 7.4424 - acc: 0.4752 - val_loss: 7.5116 - val_acc: 0.3035\n",
            "Epoch 2/3\n",
            "11240/11240 [==============================] - 71s 6ms/step - loss: 7.4417 - acc: 0.4326 - val_loss: 7.5113 - val_acc: 0.6630\n",
            "Epoch 3/3\n",
            "11240/11240 [==============================] - 71s 6ms/step - loss: 7.4418 - acc: 0.4884 - val_loss: 7.5114 - val_acc: 0.6630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnSsIdCH16yW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "751201d6-1671-4502-9342-90e59b49a1a3"
      },
      "source": [
        "results = model.evaluate(preprocessed_test_data, testbinarylabels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 3s 3ms/step\n",
            "[0.5985215977180836, 0.7209302331126013]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woIwAVvjOM5n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cc749c6a-3120-438b-f2ad-b576753c19f7"
      },
      "source": [
        "results2 = model2.evaluate(preprocessed_test_data, testmultilabels)\n",
        "\n",
        "print(results2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 2s 2ms/step\n",
            "[7.025197723299958, 0.7209302325581395]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26-1EENBiiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimise\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "model3 = models.Sequential()\n",
        "model3.add(Embedding(input_dim=vocab_size,output_dim=EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
        "#model3.add(Activation('relu'))\n",
        "model3.add(LSTM(units=100, activation='tanh'))   #change activation function to tanh\n",
        "model3.add(Dropout(0.5))\n",
        "#model3.add(layers.Dense(3, activation='softmax'))\n",
        "model3.add(layers.Dense(3, activation='sigmoid'))\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD5rDxI0jsYj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAPosCY5CN4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "803ee0fb-923e-46e2-f8dd-1e33a403e9c2"
      },
      "source": [
        "history = model3.fit(partial_X_train,\n",
        "                    partial_y_train_m,\n",
        "                    epochs=3,\n",
        "                    batch_size=100,\n",
        "                    validation_data=(X_val, y_val_m),\n",
        "                    verbose=1)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11240 samples, validate on 2000 samples\n",
            "Epoch 1/3\n",
            "11240/11240 [==============================] - 77s 7ms/step - loss: 7.4425 - acc: 0.5109 - val_loss: 7.5110 - val_acc: 0.3035\n",
            "Epoch 2/3\n",
            "11240/11240 [==============================] - 73s 7ms/step - loss: 7.4421 - acc: 0.4714 - val_loss: 7.5124 - val_acc: 0.6630\n",
            "Epoch 3/3\n",
            "11240/11240 [==============================] - 73s 6ms/step - loss: 7.4421 - acc: 0.4601 - val_loss: 7.5133 - val_acc: 0.6630\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL6D5PtvCPUp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "26f9aba1-626b-4660-883a-5fcd3d1f7e89"
      },
      "source": [
        "results = model3.evaluate(preprocessed_test_data, testmultilabels)\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "860/860 [==============================] - 2s 2ms/step\n",
            "[7.024758487523989, 0.7209302325581395]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96gVEfTpDPlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "d2db5bc8-2202-4ff7-c32c-7cea94b6a832"
      },
      "source": [
        "# from keras.layers import Dense, Conv2D, Flatten\n",
        "\n",
        "# model3 = models.Sequential()\n",
        "# model3.add(Embedding(input_dim=vocab_size,output_dim=EMBED_SIZE,input_length=MAXIMUM_LENGTH))\n",
        "# model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "# model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-5b32043ae2e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXIMUM_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    303\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUCu2NWIGMIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}