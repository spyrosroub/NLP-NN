{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab 11 Dialogue Act Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_ZORURKg-fp"
      },
      "source": [
        "# Lab 11: Dialogue Act Tagging\n",
        "\n",
        "Dialogue act (DA) tagging is an important step in the process of developing dialog systems. DA tagging is a problem usually solved by supervised machine learning approaches that all require large amounts of hand labeled data. A wide range of techniques have been investigated for DA tagging. In this lab, we explore two approaches to DA classification. We are using the Switchboard Dialog Act Corpus for training.\n",
        "Corpus can be downloaded from http://compprag.christopherpotts.net/swda.html.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ziKyA9R4gyw9"
      },
      "source": [
        "The downloaded dataset should be kept in a data folder in the same directory as this file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmTpKt_uefe5",
        "outputId": "5ec4d434-7d2a-4611-b7f1-59af0d36a7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import sklearn.metrics\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.14`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WU_9lHR71mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('swda.zip', 'r') as swda:\n",
        "   # Extract all the contents of zip file in current directory\n",
        "   swda.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6E8axaw1hAbM",
        "colab": {}
      },
      "source": [
        "f = glob.glob(\"swda/sw*/sw*.csv\")\n",
        "frames = []\n",
        "for i in range(0, len(f)):\n",
        "    frames.append(pd.read_csv(f[i]))\n",
        "\n",
        "result = pd.concat(frames, ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b7hKGF7EhM4s",
        "outputId": "12dd19b5-e8ca-491f-ddd9-2d61a94f0af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Number of converations in the dataset:\",len(result))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of converations in the dataset: 223606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ttyB2lQhc7B"
      },
      "source": [
        "The dataset has many different features, we are only using act_tag and text for this training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-jUifIdshhD0",
        "colab": {}
      },
      "source": [
        "reduced_df = result[['act_tag','text']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-iPmZvysqg2i"
      },
      "source": [
        "Reduce down the number of tags to 43 - converting the combined tags to their generic classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MQuHm0jPt_lz",
        "colab": {}
      },
      "source": [
        "# Imported from \"https://github.com/cgpotts/swda\"\n",
        "# Convert the combination tags to the generic 43 tags\n",
        "\n",
        "import re\n",
        "def damsl_act_tag(input):\n",
        "        \"\"\"\n",
        "        Seeks to duplicate the tag simplification described at the\n",
        "        Coders' Manual: http://www.stanford.edu/~jurafsky/ws97/manual.august1.html\n",
        "        \"\"\"\n",
        "        d_tags = []\n",
        "        tags = re.split(r\"\\s*[,;]\\s*\", input)\n",
        "        for tag in tags:\n",
        "            if tag in ('qy^d', 'qw^d', 'b^m'): pass\n",
        "            elif tag == 'nn^e': tag = 'ng'\n",
        "            elif tag == 'ny^e': tag = 'na'\n",
        "            else: \n",
        "                tag = re.sub(r'(.)\\^.*', r'\\1', tag)\n",
        "                tag = re.sub(r'[\\(\\)@*]', '', tag)            \n",
        "                if tag in ('qr', 'qy'):                         tag = 'qy'\n",
        "                elif tag in ('fe', 'ba'):                       tag = 'ba'\n",
        "                elif tag in ('oo', 'co', 'cc'):                 tag = 'oo_co_cc'\n",
        "                elif tag in ('fx', 'sv'):                       tag = 'sv'\n",
        "                elif tag in ('aap', 'am'):                      tag = 'aap_am'\n",
        "                elif tag in ('arp', 'nd'):                      tag = 'arp_nd'\n",
        "                elif tag in ('fo', 'o', 'fw', '\"', 'by', 'bc'): tag = 'fo_o_fw_\"_by_bc'            \n",
        "            d_tags.append(tag)\n",
        "        # Dan J says (p.c.) that it makes sense to take the first;\n",
        "        # there are only a handful of examples with 2 tags here.\n",
        "        return d_tags[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S8N_PUCAblq3",
        "outputId": "32c4570a-8cbe-4bac-ca22-eadd15ec5353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "reduced_df[\"act_tag\"] = reduced_df[\"act_tag\"].apply(lambda x: damsl_act_tag(x))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0UNy0vvhhqpD"
      },
      "source": [
        "There are 43 tags in this dataset. Some of the tags are Yes-No-Question('qy'), Statement-non-opinion('sd') and Statement-opinion('sv'). Tags information can be found here http://compprag.christopherpotts.net/swda.html#tags. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9biiyP8UiGDe"
      },
      "source": [
        "To get unique tags:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BrhW8gyLfQQK",
        "colab": {}
      },
      "source": [
        "unique_tags = set()\n",
        "for tag in reduced_df['act_tag']:\n",
        "    unique_tags.add(tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LMOX5KwgiPmu",
        "colab": {}
      },
      "source": [
        "one_hot_encoding_dic = pd.get_dummies(list(unique_tags))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZPHPCxE3iPby",
        "colab": {}
      },
      "source": [
        "tags_encoding = []\n",
        "for i in range(0, len(reduced_df)):\n",
        "    tags_encoding.append(one_hot_encoding_dic[reduced_df['act_tag'].iloc[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LVI8QyVzjqWh"
      },
      "source": [
        "The tags are one hot encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SQJTiffPjUtu"
      },
      "source": [
        "To create sentence embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PmkyD1TfjWGO",
        "colab": {}
      },
      "source": [
        "sentences = []\n",
        "for i in range(0, len(reduced_df)):\n",
        "    sentences.append(reduced_df['text'].iloc[i].split(\" \"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MlD6L6e3jV-7",
        "colab": {}
      },
      "source": [
        "wordvectors = {}\n",
        "index = 1\n",
        "for s in sentences:\n",
        "    for w in s:\n",
        "        if w not in wordvectors:\n",
        "            wordvectors[w] = index\n",
        "            index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e7_cjDHrjV1c",
        "colab": {}
      },
      "source": [
        "# Max length of 137\n",
        "MAX_LENGTH = len(max(sentences, key=len))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LX6DidEvjVWs",
        "colab": {}
      },
      "source": [
        "sentence_embeddings = []\n",
        "for s in sentences:\n",
        "    sentence_emb = []\n",
        "    for w in s:\n",
        "        sentence_emb.append(wordvectors[w])\n",
        "    sentence_embeddings.append(sentence_emb)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nr4iEyNTjmlu"
      },
      "source": [
        "Then we split the dataset into test and train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GiNZ-iI_jnOF",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, np.array(tags_encoding))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_RqMeWe_jron"
      },
      "source": [
        "And pad the sentences with zero to make all sentences of equal length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yqD7DvzRGRY7",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 137"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ai9cwv82jufe",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NxeBM_s84zu",
        "colab_type": "text"
      },
      "source": [
        "Split Train into Train and Validation - about 10% into validation - In order to validate the model as it is training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "517zYSQLXkbn",
        "colab": {}
      },
      "source": [
        "\n",
        "train_input = train_sentences_X[:140000]\n",
        "val_input = train_sentences_X[140000:]\n",
        "\n",
        "train_labels = y_train[:140000]\n",
        "val_labels = y_train[140000:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHJbZDtk7N-3"
      },
      "source": [
        "# Model 1 - \n",
        "\n",
        "The first approach we'll try is to treat DA tagging as a standard multi-class text classification task, in the way you've done before with sentiment analysis and other tasks. Each utterance will be treated independently as a text to be classified with its DA tag label. This model has an architecture of:\n",
        "\n",
        "- Embedding  \n",
        "- BLSTM  \n",
        "- Fully Connected Layer\n",
        "- Softmax Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FItlHC1Fjz6y"
      },
      "source": [
        " The model architecture is as follows: Embedding Layer (to generate word embeddings) Next layer Bidirectional LSTM. Feed forward layer with number of neurons = number of tags. Softmax activation to get the probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C_9RnNS1Ynr",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M97Sw5iv-lEU",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(wordvectors) # 43,731\n",
        "MAX_LENGTH = len(max(sentences, key=len))\n",
        "EMBED_SIZE = 100 # arbitary\n",
        "HIDDEN_SIZE = len(unique_tags) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCaX-ptaj8G2",
        "outputId": "6f9e5ec0-c1db-4d24-ae80-4220ba64875f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#Building the network\n",
        "\n",
        "# Include 2 BLSTM layers, in order to capture both the forward and backward hidden states\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE,output_dim=EMBED_SIZE,input_length=MAX_LENGTH))\n",
        "\n",
        "# Bidirectional 1\n",
        "model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))\n",
        "\n",
        "# Bidirectional 2\n",
        "model.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=False)))\n",
        "\n",
        "# Dense layer\n",
        "model.add(Dense(HIDDEN_SIZE))\n",
        "\n",
        "#model.add(TimeDistributed(Dense(1, activation='softmax')))\n",
        "# Activation\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 137, 100)          4373100   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 137, 86)           49536     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 86)                44720     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 43)                3741      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 43)                0         \n",
            "=================================================================\n",
            "Total params: 4,471,097\n",
            "Trainable params: 4,471,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeiLkgD3Arpl",
        "outputId": "7a8738f9-03b3-468a-9b55-0feb94c33728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Train the model - using validation \n",
        "\n",
        "model.fit(train_input,train_labels,validation_data=(val_input,val_labels), epochs = 3,batch_size=1000, verbose = 1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140000 samples, validate on 27704 samples\n",
            "Epoch 1/3\n",
            " 42000/140000 [========>.....................] - ETA: 7:49 - loss: 2.3205 - acc: 0.4441"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f5f0e8571cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[586,1] = 43731 is not in [0, 43731)\n\t [[{{node embedding_1/embedding_lookup}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2LkONUKQkSrL",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ab0ZL1dqkTY4",
        "colab": {}
      },
      "source": [
        "print(\"Overall Accuracy:\", score[1]*100)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LhMViQVSPY1J"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "\n",
        "The overall accuracy is 67%, an effective accuracy for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XHwoVCEwjEz7"
      },
      "source": [
        "In addition to overall accuracy, you need to look at the accuracy of some minority classes. Signal-non-understanding ('br') is a good indicator of \"other-repair\" or cases in which the other conversational participant attempts to repair the speaker's error. Summarize/reformulate ('bf') has been used in dialogue summarization. Report the accuracy for these classes and some frequent errors you notice the system makes in predicting them. What do you think the reasons are？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7owA1f27se8"
      },
      "source": [
        "## Minority Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZ8BwgDxNcIr",
        "colab": {}
      },
      "source": [
        "# Generate predictions for the test data\n",
        "y_pred = model.predict(test_sentences_X,batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl6UM7JmGKTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXVUyTvy7bwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "df_cm = pd.DataFrame(confusion_mat, index = [i for i in unique_tags],\n",
        "                  columns = [i for i in unique_tags])\n",
        "plt.figure(figsize = (15,10),)\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3TILbpNSJuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "\n",
        "print('accuracy %s' % accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))\n",
        "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1),target_names=unique_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgoHzZTUcjnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Argmax value of \"br\" as index\n",
        "br_index = np.argmax(one_hot_encoding_dic[\"br\"]) \n",
        "\n",
        "# Accuracy, using the index\n",
        "br_acc = confusion_mat[br_index][br_index] / sum(confusion_mat[br_index])\n",
        "print('\"br\" accuracy: ' + str(br_acc*100))\n",
        "\n",
        "# Argmax value of \"bf\"\n",
        "bf_index = np.argmax(one_hot_encoding_dic[\"bf\"])\n",
        "\n",
        "# Accuracy, using the index\n",
        "bf_acc = confusion_mat[bf_index][bf_index] / sum(confusion_mat[bf_index])\n",
        "print('\"bf\" accuracy: ' + str(bf_acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HdnpWLggZ-6z"
      },
      "source": [
        "\n",
        "Due to the reduced lack of training data for the minority classes, these minority classifiers will not be very confident in classification, as they have not been fully optimised. The frequent classifiers will be more optimised and will generate more confident scores for all examples, effectively crowding out the less confident minority classifiers. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BZ16sE5F7x9e"
      },
      "source": [
        "# Model 2 - Balanced Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hKHbOs4WkFaP"
      },
      "source": [
        "\n",
        "One thing we can do to try to improve performance is therefore to balance the data more sensibly. As the dataset is highly imbalanced, we can simply weight up the minority classes proportionally to their underrepresentation while training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6L4kNdf6kGEa",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "y_integers = np.argmax(tags_encoding, axis=1)\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "d_class_weights = dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF1UM-ZMZoa1"
      },
      "source": [
        "## Define & Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xIRgRAzOPSAZ",
        "colab": {}
      },
      "source": [
        "# Re-built the model for the balanced training\n",
        "\n",
        "model_balanced = Sequential()\n",
        "model_balanced.add(Embedding(input_dim=VOCAB_SIZE,output_dim=EMBED_SIZE,input_length=MAX_LENGTH))\n",
        "model_balanced.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True)))\n",
        "model_balanced.add(Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=False)))\n",
        "model_balanced.add(Dense(HIDDEN_SIZE))\n",
        "model_balanced.add(Activation('softmax'))\n",
        "\n",
        "model_balanced.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model_balanced.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xB2McUREkL4B",
        "colab": {}
      },
      "source": [
        "# Train the balanced network -  takes  time to achieve good accuracy\n",
        "for i in range(10):\n",
        "  model_balanced.fit(train_input,train_labels,validation_data=(val_input,val_labels), epochs = 5,batch_size=1000, verbose = 1,class_weight=d_class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DJPjlMclZtw2"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8UMAMGpJRINC",
        "colab": {}
      },
      "source": [
        "# Overall Accuracy\n",
        "score = model_balanced.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0xzLIkTarjei",
        "colab": {}
      },
      "source": [
        "print(\"Overall Accuracy:\", score[1]*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qkULcz2igEW3",
        "colab": {}
      },
      "source": [
        "# Generate predictions for the test data\n",
        "label_pred = model_balanced.predict(test_sentences_X, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hq7i7giWZ4_l"
      },
      "source": [
        "## Balanced network evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fM7VWweco0Et"
      },
      "source": [
        "Report the overall accuracy and the accuracy of  'br' and 'bf'  classes. Suggest other ways to handle imbalanced classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jNfWmSNgRvT",
        "colab": {}
      },
      "source": [
        "# Build the confusion matrix off these predictions\n",
        "\n",
        "matrix_balanced = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1))\n",
        "\n",
        "df_cm = pd.DataFrame(matrix_balanced, index = [i for i in unique_tags],\n",
        "                  columns = [i for i in unique_tags])\n",
        "plt.figure(figsize = (15,10),)\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmdBW3FOFiyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('accuracy %s' % accuracy_score(np.argmax(y_test, axis=1), np.argmax(label_pred, axis=1)))\n",
        "print(classification_report(np.argmax(y_test, axis=1), np.argmax(label_pred, axis=1),target_names=unique_tags))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvxzeql_dI2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"br\"\n",
        "br_index = np.argmax(one_hot_encoding_dic[\"br\"])\n",
        "br_acc = matrix_balanced[br_index][br_index] / sum(matrix_balanced[br_index])\n",
        "print('\"br\" accuracy: ' + str(br_acc*100))\n",
        "\n",
        "\"bf\"\n",
        "bf_index = np.argmax(one_hot_encoding_dic[\"bf\"])\n",
        "bf_acc = matrix_balanced[bf_index][bf_index] / sum(matrix_balanced[bf_index])\n",
        "print('\"bf\" accuracy: ' + str(bf_acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zi9GyVUvPcrF"
      },
      "source": [
        "\n",
        "\n",
        "### Accuracies\n",
        "\n",
        "\n",
        "\n",
        "### Explanation\n",
        "\n",
        "\n",
        "### Other ways to handle imbalanced classes\n",
        "\n",
        "\n",
        "- \n",
        "\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fW4g5mQkkaFv"
      },
      "source": [
        "Can we improve things by using context information?  Next we try to build a model which predicts DA tag from the sequence of \n",
        "previous DA tags, plus the utterance representation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WfrGWuZ6nk4y"
      },
      "source": [
        "# Using Context for Dialog Act Classification\n",
        "\n",
        "The second approach we will try is a hierarchical approach to DA tagging. We expect there is valuable sequential information among the DA tags. So in this section we apply a BiLSTM on top of the sentence CNN representation. The CNN model learns textual information in each utterance for DA classification, acting like the text classifier from Model 1 above. Then we use a bidirectional-LSTM (BLSTM) above that to learn how to use the context before and after the current utterance to improve the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7qyPpNaK-2mb"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "This model has an architecture of:\n",
        "\n",
        "- Word Embedding\n",
        "- CNN\n",
        "- Bidirectional LSTM\n",
        "- Fully-Connected output\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DuJLqgjWqcIf"
      },
      "source": [
        "## CNN\n",
        "\n",
        "\n",
        "This is a classical CNN layer used to convolve over embedings tensor and gether useful information from it. The data is represented by hierarchy of features, which can be modelled using a CNN. We transform/reshape conv output to 2d matrix. Then we pass it to the max pooling layer that applies the max pool operation on windows of different sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA5INtFl-fM0",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input,Reshape,Conv2D,MaxPool2D,BatchNormalization,Flatten\n",
        "\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 64\n",
        "drop = 0.2\n",
        "VOCAB_SIZE = len(wordvectors) # 43,731\n",
        "MAX_LENGTH = len(max(sentences, key=len))\n",
        "EMBED_SIZE = 100 # arbitary\n",
        "HIDDEN_SIZE = len(unique_tags) \n",
        "\n",
        "# CNN model\n",
        "inputs = Input(shape=(MAX_LENGTH, ), dtype='int32')\n",
        "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_LENGTH)(inputs)\n",
        "reshape = Reshape((MAX_LENGTH, EMBED_SIZE, 1))(embedding)\n",
        "\n",
        "# 3 convolutions\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "bn_0 = BatchNormalization()(conv_0)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "bn_1 = BatchNormalization()(conv_1)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "bn_2 = BatchNormalization()(conv_2)\n",
        "\n",
        "# maxpool for 3 layers\n",
        "maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), padding='valid')(bn_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), padding='valid')(bn_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), padding='valid')(bn_2)\n",
        "\n",
        "# concatenate tensors\n",
        "merge = keras.layers.concatenate([maxpool_0,maxpool_1,maxpool_2])\n",
        "# flatten concatenated tensors\n",
        "flat = Flatten()(merge)\n",
        "# dense layer (dense_1)\n",
        "dense_1 = Dense(HIDDEN_SIZE)(flat)\n",
        "# dropout_1\n",
        "dropout_1 = Dropout(drop)(dense_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wDuERMw7-rAV"
      },
      "source": [
        "## BLSTM\n",
        "\n",
        "This is used to create LSTM layers. The data we’re working with has temporal properties which we want to model as well — hence the use of a LSTM. You should create a BiLSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pFGp2EWI-fM7",
        "colab": {}
      },
      "source": [
        "# BLSTM model\n",
        "\n",
        "# Bidirectional 1\n",
        "BLSTM1 = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True))(embedding)\n",
        "\n",
        "# Bidirectional 2\n",
        "BLSTM2 = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=False))(BLSTM1)\n",
        "\n",
        "# Dense layer (dense_2)\n",
        "dense_2 = Dropout(drop)(BLSTM2)\n",
        "\n",
        "# dropout_2\n",
        "dropout_2 = Dropout(drop)(dense_2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7wluAkx6AQUb"
      },
      "source": [
        "Concatenate 2 last layers and create the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kzrhgkX2-fNE",
        "colab": {}
      },
      "source": [
        "# concatenate 2 final layers\n",
        "\n",
        "final = keras.layers.concatenate([dropout_1, dropout_2])\n",
        "\n",
        "# output\n",
        "output = Dense(units=HIDDEN_SIZE, activation='softmax')(final)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Jneg-GD-fNJ",
        "colab": {}
      },
      "source": [
        "from keras import Model\n",
        "\n",
        "# Train the model - using validation \n",
        "model = keras.Model(inputs=inputs,outputs=output)\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(train_input,train_labels,validation_data=(val_input,val_labels), epochs = 5,batch_size=1000, verbose = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MSMRSX1u-fNO",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3qFMsXNS-fNS",
        "colab": {}
      },
      "source": [
        "print(\"Overall Accuracy:\", score[1]*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kgoRnWIeEts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions for the test data\n",
        "label_pred = model.predict(test_sentences_X, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXgKXSsjeVSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1))\n",
        "\n",
        "\"br\"\n",
        "br_index = np.argmax(one_hot_encoding_dic[\"br\"])\n",
        "br_acc = matrix[br_index][br_index] / sum(matrix[br_index])\n",
        "print('\"br\" accuracy: ' + str(br_acc*100))\n",
        "\n",
        "\"bf\"\n",
        "bf_index = np.argmax(one_hot_encoding_dic[\"bf\"])\n",
        "bf_acc = matrix[bf_index][bf_index] / sum(matrix[bf_index])\n",
        "print('\"bf\" accuracy: ' + str(bf_acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RKMmrfuisKGJ"
      },
      "source": [
        "Report your overall accuracy. Did context help disambiguate and better predict the minority classes ('br' and 'bf')? What are frequent errors? Show one positive example where adding context changed the prediction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QmO6hVsWTaNr"
      },
      "source": [
        "### Minority Classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gUZt48JgrE34"
      },
      "source": [
        "# Advanced:  Bert-Based Model for Dialogue Act Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE63Q5guuPdA"
      },
      "source": [
        "In the last section we want to use BERT and leverage contextual word embeddings, following on from the last lab you've \n",
        "just done. This is an advanced part of the assignment and worth 10 marks (20%) in total. You could use your BERT-based text classifier here (instead of the CNN utterance-level classifier) and see if a pre-trained BERT language model helps. The domain difference from conversational data is one possible downside to using BERT. Explore some techniques to efficiently transfer the knowledge from conversational data and to improve model performance on DA tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBSiZZO3ih0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}