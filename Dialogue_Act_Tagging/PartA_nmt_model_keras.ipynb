{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "PartA_nmt_model_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cep3kKTbfVb4",
        "colab_type": "code",
        "outputId": "456332d6-b7bc-4efe-f062-1f55190d358d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Embedding,LSTM,Dropout,Dense,Layer,multiply\n",
        "from keras import Model,Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import collections\n",
        "import numpy as np\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "class NmtModel(object):\n",
        "  def __init__(self,source_dict,target_dict,use_attention):\n",
        "    self.hidden_size = 200\n",
        "    self.embedding_size = 100\n",
        "    self.hidden_dropout_rate=0.2\n",
        "    self.embedding_dropout_rate = 0.2\n",
        "    self.batch_size = 100\n",
        "    self.max_target_step = 30\n",
        "    self.vocab_target_size = len(target_dict.vocab)\n",
        "    self.vocab_source_size = len(source_dict.vocab)\n",
        "    self.target_dict = target_dict\n",
        "    self.source_dict = source_dict\n",
        "    self.SOS = target_dict.word2ids['<start>']\n",
        "    self.EOS = target_dict.word2ids['<end>']\n",
        "    self.use_attention = use_attention\n",
        "\n",
        "    print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    source_words = Input(shape=(None,),dtype='int32')\n",
        "    target_words = Input(shape=(None,), dtype='int32')\n",
        "\n",
        "    \"\"\"\n",
        "    Task 1 encoder\n",
        "    \n",
        "    Start\n",
        "    \"\"\"\n",
        "    #create two Embedding layers\n",
        "    embedding_source = Embedding(input_dim = self.vocab_source_size,output_dim = self.embedding_size,\n",
        "                                 mask_zero=True)\n",
        "    \n",
        "    embedding_target = Embedding(input_dim = self.vocab_target_size,output_dim = self.embedding_size,\n",
        "                                mask_zero=True)\n",
        "    \n",
        "    #passing the inputs through the Embedding layers                            \n",
        "    source_words_embeddings = embedding_source(source_words)\n",
        "    target_words_embeddings = embedding_target(target_words)\n",
        "    \n",
        "    #apply dropout to the embeddings\n",
        "    source_words_embeddings = Dropout(self.embedding_dropout_rate)(source_words_embeddings)\n",
        "    target_words_embeddings = Dropout(self.embedding_dropout_rate)(target_words_embeddings)\n",
        "    \n",
        "    #create a LSTM layer to process the source_words_embeddings\n",
        "    encoder_outputs,encoder_state_h,encoder_state_c = LSTM(self.hidden_size,recurrent_dropout=self.hidden_dropout_rate,return_sequences=True,return_state=True)(source_words_embeddings)\n",
        "    \n",
        "    \"\"\"\n",
        "    End Task 1\n",
        "    \"\"\"\n",
        "    encoder_states = [encoder_state_h,encoder_state_c]\n",
        "\n",
        "    decoder_lstm = LSTM(self.hidden_size,recurrent_dropout=self.hidden_dropout_rate,return_sequences=True,return_state=True)\n",
        "    decoder_outputs_train,_,_ = decoder_lstm(target_words_embeddings,initial_state=encoder_states)\n",
        "\n",
        "\n",
        "    if self.use_attention:\n",
        "      decoder_attention = AttentionLayer()\n",
        "      decoder_outputs_train = decoder_attention([encoder_outputs,decoder_outputs_train])\n",
        "\n",
        "    decoder_dense = Dense(self.vocab_target_size,activation='softmax')\n",
        "    decoder_outputs_train = decoder_dense(decoder_outputs_train)\n",
        "\n",
        "    adam = Adam(lr=0.01,clipnorm=5.0)\n",
        "    self.train_model = Model([source_words,target_words], decoder_outputs_train)\n",
        "    self.train_model.compile(optimizer=adam,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.train_model.summary()\n",
        "\n",
        "    #Inference Models\n",
        "\n",
        "    self.encoder_model = Model(source_words,[encoder_outputs,encoder_state_h,encoder_state_c])\n",
        "    self.encoder_model.summary()\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(self.hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(self.hidden_size,))\n",
        "    encoder_outputs_input = Input(shape=(None,self.hidden_size,))\n",
        "\n",
        "    \"\"\"\n",
        "    Task 2 decoder for inference\n",
        "    \n",
        "    Start\n",
        "    \"\"\"\n",
        "    #create a list with the decoder_states\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    # pass the target_word_embeddings and decoder_states to the decoder_lstm\n",
        "    decoder_outputs,decoder_state_output_h,decoder_state_output_c = decoder_lstm(target_words_embeddings,initial_state=decoder_states_inputs)\n",
        "    \n",
        "    #attention model\n",
        "    if self.use_attention:\n",
        "      decoder_attention = AttentionLayer()\n",
        "      #decoder_outputs_test = decoder_attention([decoder_outputs_train,decoder_outputs_test])\n",
        "      decoder_outputs = decoder_attention([encoder_outputs_input,decoder_outputs])\n",
        "\n",
        "    #pass the output into the final layer of the decoder (decoder_dense)\n",
        "    decoder_outputs_test = decoder_dense(decoder_outputs)\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    End Task 2 \n",
        "    \"\"\"\n",
        "\n",
        "    self.decoder_model = Model([target_words,decoder_state_input_h,decoder_state_input_c,encoder_outputs_input],\n",
        "                               [decoder_outputs_test,decoder_state_output_h,decoder_state_output_c])\n",
        "    self.decoder_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "  def time_used(self, start_time):\n",
        "    curr_time = time.time()\n",
        "    used_time = curr_time-start_time\n",
        "    m = used_time // 60\n",
        "    s = used_time - 60 * m\n",
        "    return \"%d m %d s\" % (m, s)\n",
        "\n",
        "  def train(self,train_data,dev_data,test_data, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      epoch_time = time.time()\n",
        "      source_words_train, target_words_train, target_words_train_labels = train_data\n",
        "\n",
        "      self.train_model.fit([source_words_train,target_words_train],target_words_train_labels,batch_size=self.batch_size)\n",
        "\n",
        "      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "      dev_time = time.time()\n",
        "      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "      self.eval(dev_data)\n",
        "      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "    print(\"Evaluating on test set:\")\n",
        "    test_time = time.time()\n",
        "    self.eval(test_data)\n",
        "    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "\n",
        "\n",
        "  def get_target_sentences(self, sents,vocab,reference=False):\n",
        "    str_sents = []\n",
        "    num_sent, max_len = sents.shape\n",
        "    for i in range(num_sent):\n",
        "      str_sent = []\n",
        "      for j in range(max_len):\n",
        "        t = sents[i,j].item()\n",
        "        if t == self.SOS:\n",
        "          continue\n",
        "        if t == self.EOS:\n",
        "          break\n",
        "\n",
        "        str_sent.append(vocab[t])\n",
        "      if reference:\n",
        "        str_sents.append([str_sent])\n",
        "      else:\n",
        "        str_sents.append(str_sent)\n",
        "    return str_sents\n",
        "\n",
        "\n",
        "  def eval(self, dataset):\n",
        "    source_words, target_words_labels = dataset\n",
        "    vocab = self.target_dict.vocab\n",
        "\n",
        "    encoder_outputs, state_h,state_c = self.encoder_model.predict(source_words,batch_size=self.batch_size)\n",
        "    predictions = []\n",
        "    step_target_words = np.ones([source_words.shape[0],1]) * self.SOS\n",
        "    for _ in range(self.max_target_step):\n",
        "      step_decoder_outputs, state_h,state_c = self.decoder_model.predict([step_target_words,state_h,state_c,encoder_outputs],batch_size=self.batch_size)\n",
        "      step_target_words = np.argmax(step_decoder_outputs,axis=2)\n",
        "      predictions.append(step_target_words)\n",
        "\n",
        "    candidates = self.get_target_sentences(np.concatenate(predictions,axis=1),vocab)\n",
        "    references = self.get_target_sentences(target_words_labels,vocab,reference=True)\n",
        "\n",
        "    score = corpus_bleu(references,candidates)\n",
        "    print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    if mask == None:\n",
        "      return None\n",
        "    return mask[1]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[1][0],input_shape[1][1],input_shape[1][2]*2)\n",
        "\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    encoder_outputs, decoder_outputs = inputs\n",
        "\n",
        "    \"\"\"\n",
        "    Task 3 attention\n",
        "    \n",
        "    Start\n",
        "    \"\"\"\n",
        "    # transpose the last two dimensions \n",
        "    decoder_out = K.permute_dimensions(decoder_outputs,(0, 2, 1))\n",
        "    \n",
        "    #do the multiplications\n",
        "    luong_score = K.batch_dot(encoder_outputs, decoder_out)\n",
        "\n",
        "    # apply a softmax\n",
        "    luong_score = K.softmax(luong_score,axis=1)\n",
        "\n",
        "    # expand dimensions\n",
        "    luong_score = K.expand_dims(luong_score,axis= 3)\n",
        "    encoder_outputs = K.expand_dims(encoder_outputs,axis= 2)\n",
        "\n",
        "    #multiply the two tensors and sum max_source_sent_len dimension to create the encoder_vector\n",
        "    encoder_vector = multiply([encoder_outputs,luong_score])\n",
        "    #encoder_vector = np.dot(encoder_outputs,luong_score)\n",
        "    encoder_vector = K.sum(encoder_vector,axis=1)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    End Task 3\n",
        "    \"\"\"\n",
        "    # [batch,max_dec,2*emb]\n",
        "    new_decoder_outputs = K.concatenate([decoder_outputs, encoder_vector])\n",
        "\n",
        "    return new_decoder_outputs\n",
        "\n",
        "\n",
        "class LanguageDict():\n",
        "  def __init__(self, sents):\n",
        "    word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "    self.vocab = []\n",
        "    self.vocab.append('<pad>') #zero paddings\n",
        "    self.vocab.append('<unk>')\n",
        "    self.vocab.extend([t for t,c in word_counter.items() if c > 10])\n",
        "\n",
        "    self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "    self.UNK = self.word2ids['<unk>']\n",
        "    self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(source_path,target_path, max_num_examples=30000):\n",
        "  source_lines = open(source_path).readlines()\n",
        "  target_lines = open(target_path).readlines()\n",
        "  assert len(source_lines) == len(target_lines)\n",
        "  if max_num_examples > 0:\n",
        "    max_num_examples = min(len(source_lines), max_num_examples)\n",
        "    source_lines = source_lines[:max_num_examples]\n",
        "    target_lines = target_lines[:max_num_examples]\n",
        "\n",
        "  source_sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in source_lines]\n",
        "  target_sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in target_lines]\n",
        "  for sent in target_sents:\n",
        "    sent.append('<end>')\n",
        "    sent.insert(0,'<start>')\n",
        "\n",
        "  source_lang_dict = LanguageDict(source_sents)\n",
        "  target_lang_dict = LanguageDict(target_sents)\n",
        "\n",
        "  unit = len(source_sents)//10\n",
        "\n",
        "  source_words = [[source_lang_dict.word2ids.get(tok,source_lang_dict.UNK) for tok in sent] for sent in source_sents]\n",
        "  source_words_train = pad_sequences(source_words[:8*unit],padding='post')\n",
        "  source_words_dev = pad_sequences(source_words[8*unit:9*unit],padding='post')\n",
        "  source_words_test = pad_sequences(source_words[9*unit:],padding='post')\n",
        "\n",
        "  eos = target_lang_dict.word2ids['<end>']\n",
        "\n",
        "  target_words = [[target_lang_dict.word2ids.get(tok,target_lang_dict.UNK) for tok in sent[:-1]] for sent in target_sents]\n",
        "  target_words_train = pad_sequences(target_words[:8*unit],padding='post')\n",
        "\n",
        "  target_words_train_labels = [sent[1:]+[eos] for sent in target_words[:8*unit]]\n",
        "  target_words_train_labels = pad_sequences(target_words_train_labels,padding='post')\n",
        "  target_words_train_labels = np.expand_dims(target_words_train_labels,axis=2)\n",
        "\n",
        "  target_words_dev_labels = pad_sequences([sent[1:] + [eos] for sent in target_words[8 * unit:9 * unit]], padding='post')\n",
        "  target_words_test_labels = pad_sequences([sent[1:] + [eos] for sent in target_words[9 * unit:]], padding='post')\n",
        "\n",
        "  train_data = [source_words_train,target_words_train,target_words_train_labels]\n",
        "  dev_data = [source_words_dev,target_words_dev_labels]\n",
        "  test_data = [source_words_test,target_words_test_labels]\n",
        "\n",
        "  return train_data,dev_data,test_data,source_lang_dict,target_lang_dict\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  max_example = 30000\n",
        "  use_attention = True\n",
        "  train_data, dev_data, test_data, source_dict, target_dict = load_dataset(\"data.30.vi\",\"data.30.en\",max_num_examples=max_example)\n",
        "  print(\"read %d/%d/%d train/dev/test batches\" % (len(train_data[0]),len(dev_data[0]), len(test_data[0])))\n",
        "\n",
        "  model = NmtModel(source_dict,target_dict,use_attention)\n",
        "  model.build()\n",
        "  model.train(train_data,dev_data,test_data,10)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "read 24000/3000/3000 train/dev/test batches\n",
            "source vocab: 2034, target vocab:2506\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    203400      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, None, 100)    0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 100)    250600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  240800      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, None, 100)    0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 200),  240800      dropout_2[0][0]                  \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer_1 (AttentionLay (None, None, 400)    0           lstm_1[0][0]                     \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 2506)   1004906     attention_layer_1[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 1,940,506\n",
            "Trainable params: 1,940,506\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 100)         203400    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                [(None, None, 200), (None 240800    \n",
            "=================================================================\n",
            "Total params: 444,200\n",
            "Trainable params: 444,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 100)    250600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, None, 100)    0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 200)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            (None, 200)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, None, 200)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 200),  240800      dropout_2[0][0]                  \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer_2 (AttentionLay (None, None, 400)    0           input_5[0][0]                    \n",
            "                                                                 lstm_2[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 2506)   1004906     attention_layer_2[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 1,496,306\n",
            "Trainable params: 1,496,306\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Starting training epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "24000/24000 [==============================] - 280s 12ms/step - loss: 4.2843 - acc: 0.2722\n",
            "Time used for epoch 1: 4 m 42 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 5.10\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 2/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 276s 11ms/step - loss: 3.3130 - acc: 0.3956\n",
            "Time used for epoch 2: 4 m 35 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 9.91\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 3/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 275s 11ms/step - loss: 2.8213 - acc: 0.4471\n",
            "Time used for epoch 3: 4 m 35 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 12.58\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 4/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 275s 11ms/step - loss: 2.5370 - acc: 0.4752\n",
            "Time used for epoch 4: 4 m 35 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 13.36\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 5/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 275s 11ms/step - loss: 2.3632 - acc: 0.4927\n",
            "Time used for epoch 5: 4 m 35 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 14.16\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 6/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 274s 11ms/step - loss: 2.2414 - acc: 0.5064\n",
            "Time used for epoch 6: 4 m 34 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 14.41\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 7/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 274s 11ms/step - loss: 2.1485 - acc: 0.5175\n",
            "Time used for epoch 7: 4 m 33 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 14.61\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 8/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 276s 12ms/step - loss: 2.0808 - acc: 0.5256\n",
            "Time used for epoch 8: 4 m 36 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 14.47\n",
            "Time used for evaluate on dev set: 0 m 13 s\n",
            "Starting training epoch 9/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 278s 12ms/step - loss: 2.0258 - acc: 0.5327\n",
            "Time used for epoch 9: 4 m 38 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 14.74\n",
            "Time used for evaluate on dev set: 0 m 14 s\n",
            "Starting training epoch 10/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 276s 12ms/step - loss: 1.9785 - acc: 0.5388\n",
            "Time used for epoch 10: 4 m 36 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 14.91\n",
            "Time used for evaluate on dev set: 0 m 15 s\n",
            "Training finished!\n",
            "Time used for training: 48 m 27 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 15.07\n",
            "Time used for evaluate on test set: 0 m 15 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s93J2ndLfVcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}