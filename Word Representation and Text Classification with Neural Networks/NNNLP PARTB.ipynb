{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab4_text_classification (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 4 - Neural Network Classifier Using Simple Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to solve a text classification task using simple \n",
        "feedforward neural network classifier. We will use IMDB Large Movie Review Dataset to train a binary classification model, able to predict whether a review is positive or negative. First, our network takes one-hot word vectors as input, averages them to make one vector and trains a \n",
        "fully-connected layer to predict the output. In the second part, we replace the one-hot vectors with the word embeddings and add a layer to see how much that improves the performance.\n",
        "\n",
        "We are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. But it is not straightforward to define models where layers connect to more than just the previous and next layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8fpBfhBpupy",
        "outputId": "74406e71-3050-46c4-c94a-bbbe3298bf27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EundMtGPpCdf"
      },
      "source": [
        "The dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 50000 labeled movie reviews. These are split into 25,000 reviews for training and 25,000 reviews for testing. The  dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The data is preprocessed. For text classification, it is ususal to limit the size of the vocabulary to stop the dataset from becoming too sparse, creating possible overfitting. We keep the top 10,000 most frequently occurring words in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NyuSzkafqNca",
        "outputId": "093b5bc6-9eb2-4d42-a75f-74c60b4025ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see the length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-gjWRAuqg5s",
        "outputId": "80b0bf43-5733-4167-a0f6-7e3624728bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(X_train), len(y_train)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "The  reviews have been converted to integers and each integer represents a  word in a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "79Ev72Kgq4XL",
        "outputId": "6c320422-007f-443a-d10f-f4b5e9d741f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " X_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "We can convert integers back to words by querying a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMCH1OoDrSNR",
        "outputId": "75f4e326-da9c-41f1-92f7-c6b84a35a1e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2ZDP3Gnn-IJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5IreFXgruZot"
      },
      "source": [
        "Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown tokens. Index 0 will be used for padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abIb7Fe5u3GQ",
        "colab": {}
      },
      "source": [
        "\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<UNUSED>\"] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "To reverse key and values in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKOiVVXQu-_I",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZmTJEm8xvUvW"
      },
      "source": [
        "To view a word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SqN5jgVKvJJZ",
        "outputId": "45834496-807b-4e5e-d2d6-8e8ba6482db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "reverse_word_index[25]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6QjrzgVvrYn"
      },
      "source": [
        "And to recreate the whole sentence from our training data we define decode_review:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvrKeMgxvWlv",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sxg4YA_NvdRg",
        "outputId": "70c6de05-0703-4b6b-b878-9f307ac82f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "decode_review(X_train[10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c8gIzXncfaJK"
      },
      "source": [
        "### Creating One-hot word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9W4yb3rv_E0"
      },
      "source": [
        "It is  common to use one-hot representation as input in Natural Language Processing tasks. In Keras, the Embedding layer takes an index as an input and convert it to one-hot vector with the length of the vocabulary size. Then multiplies these vectors by a normal weight matrix. But there is no way to only get a one-hot vector as the output of a layer in Keras. To solve this we use Lambda() layer and a function that creates the one-hot layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RPO_pK9zH4C5",
        "colab": {}
      },
      "source": [
        "def OneHot(input_dim=None, input_length=None):\n",
        "    \n",
        "    if input_dim is None or input_length is None:\n",
        "        raise TypeError(\"input_dim or input_length is not set\")\n",
        "\n",
        "    \n",
        "    def _one_hot(x, num_classes):\n",
        "        return K.one_hot(K.cast(x, 'uint8'),\n",
        "                          num_classes=num_classes)\n",
        "\n",
        "    return Lambda(_one_hot,\n",
        "                  arguments={'num_classes': input_dim},\n",
        "                  input_shape=(input_length,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "364d3MAw0ez9"
      },
      "source": [
        "input_dim refers to the length of the one-hot vector and input_length refers to the length of the input sequence. Since the input to K.one_hot should be an integer tensor, we cast x to one (Keras passes around float tensors by default).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VHz76GNA2M4r"
      },
      "source": [
        " Each text sequence has in most cases different length of words. Here, we fill sequences with a pad token (0) to fit the size. This special tokens is then masked not to be accounted in averaging, loss calculation etc. We set the maximum length to 256."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9G_o7PsvgSFt"
      },
      "source": [
        "### Preparing input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jiFn7sd_wF5j",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "X_train_enc = keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "X_test_enc = keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kcjFH1wKF_7d"
      },
      "source": [
        "And to view a padded review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zwH4dcfW_a18",
        "outputId": "f781a527-4dc9-48e2-9ebe-32704dad85d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "print(X_train_enc[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
            "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
            "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
            "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
            "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
            "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
            "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
            "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
            " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
            "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
            "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
            " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
            "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
            "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1zcxFwNGepA"
      },
      "source": [
        "Now we want to build the neural network model. We  are going to have a hidden layer with 16 hidden units. \n",
        "\n",
        "First, we want to transform each index to an embedded vector and then average all vectors to a single one. It has been showed that unweighted average of word vectors outperforms many complicated networks that model semantic and syntactic compositionality. As an example you can take a look at this: (http://anthology.aclweb.org/P/P15/P15-1162.pdf)\n",
        "\n",
        "To average we need to ignore padded zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yi04MLIvJOGZ",
        "colab": {}
      },
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "whgIIB5ggjna"
      },
      "source": [
        "### Neural Network model using one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jlOLnlnSJgrU"
      },
      "source": [
        "The first layer is an one-hot layer. \n",
        "The second layer is to compute average on all word vectors in a sentence without considering padding.\n",
        "The  output vector is piped through a fully-connected layer. \n",
        "The last layer is connected with a single output node with the sigmoid activation function. The final value is a float between 0 and 1.\n",
        "\n",
        "> Indented block\n",
        "\n",
        "\n",
        "The vocabulary count of the movie reviews (10000) is used as the input shape. At the end we visualize the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk0JTlNCFviW",
        "colab_type": "code",
        "outputId": "b48a6096-71eb-4585-9e10-f58f6f4beb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(OneHot(input_dim=VOCAB_SIZE,input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(GlobalAveragePooling1DMasked())\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Mz96xpCgvTj"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F3HbW_IKLqwT"
      },
      "source": [
        "To compile the model we need a loss function and an optimizer. We use binary_crossentropy loss function which is just a special case of categorical cross entropy. We also use Adam optimizer that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can read more about it here:\n",
        "(https://arxiv.org/abs/1412.6980v8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qh1PWTNMxjUw",
        "outputId": "cd02a973-ec7d-46b1-ff2f-295efebfc2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1jwQQqCN5Ia"
      },
      "source": [
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5lAqzQlxjSM",
        "colab": {}
      },
      "source": [
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "y_val = np.array(y_train[:10000])#\n",
        "partial_y_train = np.array(y_train[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E8Kpo5G3OJEY"
      },
      "source": [
        "Then we start to train the model for 40 epochs in mini-batches of 512 samples and monitor the model's loss and accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "99_z39KAxjPi",
        "outputId": "629233ab-dd88-485a-cbba-8549eacb1ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "15000/15000 [==============================] - 12s 826us/step - loss: 0.6930 - acc: 0.5005 - val_loss: 0.6928 - val_acc: 0.4947\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6925 - acc: 0.5035 - val_loss: 0.6924 - val_acc: 0.4952\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6921 - acc: 0.5197 - val_loss: 0.6920 - val_acc: 0.5502\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.6917 - acc: 0.5563 - val_loss: 0.6916 - val_acc: 0.5661\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6913 - acc: 0.6174 - val_loss: 0.6913 - val_acc: 0.6171\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6909 - acc: 0.6428 - val_loss: 0.6909 - val_acc: 0.6308\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6905 - acc: 0.6319 - val_loss: 0.6905 - val_acc: 0.6206\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6901 - acc: 0.6162 - val_loss: 0.6902 - val_acc: 0.6108\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6897 - acc: 0.6426 - val_loss: 0.6898 - val_acc: 0.6357\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6892 - acc: 0.6338 - val_loss: 0.6894 - val_acc: 0.6271\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6889 - acc: 0.6349 - val_loss: 0.6891 - val_acc: 0.6229\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6884 - acc: 0.6511 - val_loss: 0.6886 - val_acc: 0.6550\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6880 - acc: 0.6610 - val_loss: 0.6883 - val_acc: 0.6570\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6877 - acc: 0.6684 - val_loss: 0.6879 - val_acc: 0.6607\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6873 - acc: 0.6711 - val_loss: 0.6875 - val_acc: 0.6579\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.6869 - acc: 0.6677 - val_loss: 0.6872 - val_acc: 0.6587\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6865 - acc: 0.6659 - val_loss: 0.6868 - val_acc: 0.6598\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6861 - acc: 0.6733 - val_loss: 0.6864 - val_acc: 0.6642\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6857 - acc: 0.6722 - val_loss: 0.6861 - val_acc: 0.6634\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6853 - acc: 0.6717 - val_loss: 0.6857 - val_acc: 0.6654\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6849 - acc: 0.6754 - val_loss: 0.6854 - val_acc: 0.6651\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.6846 - acc: 0.6750 - val_loss: 0.6850 - val_acc: 0.6657\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6842 - acc: 0.6763 - val_loss: 0.6846 - val_acc: 0.6663\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6838 - acc: 0.6721 - val_loss: 0.6843 - val_acc: 0.6663\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6834 - acc: 0.6759 - val_loss: 0.6840 - val_acc: 0.6651\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6830 - acc: 0.6776 - val_loss: 0.6836 - val_acc: 0.6664\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6827 - acc: 0.6769 - val_loss: 0.6833 - val_acc: 0.6653\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6823 - acc: 0.6757 - val_loss: 0.6829 - val_acc: 0.6661\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6820 - acc: 0.6769 - val_loss: 0.6826 - val_acc: 0.6681\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6816 - acc: 0.6763 - val_loss: 0.6823 - val_acc: 0.6672\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6812 - acc: 0.6779 - val_loss: 0.6819 - val_acc: 0.6701\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6809 - acc: 0.6787 - val_loss: 0.6815 - val_acc: 0.6688\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 3s 194us/step - loss: 0.6805 - acc: 0.6790 - val_loss: 0.6812 - val_acc: 0.6699\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6801 - acc: 0.6781 - val_loss: 0.6808 - val_acc: 0.6696\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6798 - acc: 0.6781 - val_loss: 0.6806 - val_acc: 0.6688\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6794 - acc: 0.6794 - val_loss: 0.6802 - val_acc: 0.6697\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6790 - acc: 0.6800 - val_loss: 0.6798 - val_acc: 0.6704\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6787 - acc: 0.6813 - val_loss: 0.6795 - val_acc: 0.6703\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6783 - acc: 0.6812 - val_loss: 0.6791 - val_acc: 0.6698\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6779 - acc: 0.6809 - val_loss: 0.6788 - val_acc: 0.6702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i_9a_rybhG5J"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYLH8kOgOo9W"
      },
      "source": [
        "To evaulate the model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFMt2Q7b3taP",
        "outputId": "7eeb4f4c-da0c-4aba-dfd7-7c7514740cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 4s 142us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laxM1B9OvT0L",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RrKiPHcAmQU",
        "outputId": "2d08bde9-d245-48bd-c900-6a799083c470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(results)\n",
        "# loss, accuracay "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6785999244117736, 0.67592]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGcRvLMx2vtu",
        "colab_type": "code",
        "outputId": "ff286c24-a127-4dd7-a9b0-49a3a159b0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 256, 10000)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 10001     \n",
            "=================================================================\n",
            "Total params: 10,001\n",
            "Trainable params: 10,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pW7IpHxMO6qp"
      },
      "source": [
        "Our first model accuracy using one-hot vectors is \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwZk_yoWhPJB"
      },
      "source": [
        "### Plotting the accuracy graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIDPH1J7PMzN"
      },
      "source": [
        "To plot a graph of accuracy and loss over time we can use Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LS9k2vvSAqB7",
        "outputId": "ac79b585-2356-4eca-baeb-6c255e7720db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nExVLHMYYY6Ly/2go5Hiqq8LsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "### Neural Network model using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now instead of one-hot vectors, we want to use embedding. We change our first layer in model1 to an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFrCsL-NBFVL",
        "outputId": "860c5a34-6f0b-4a77-b4a7-173f7a18fdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "VOCAB_SIZE= 10000\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model2 = models.Sequential()\n",
        "model2.add(Embedding(input_dim=VOCAB_SIZE,output_dim=16,input_length=MAX_SEQUENCE_LENGTH))\n",
        "model2.add(GlobalAveragePooling1DMasked())\n",
        "model2.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history2 = model2.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model2.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 0s 24us/step - loss: 0.6915 - acc: 0.6098 - val_loss: 0.6897 - val_acc: 0.6868\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.6873 - acc: 0.7193 - val_loss: 0.6855 - val_acc: 0.7240\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.6819 - acc: 0.7466 - val_loss: 0.6800 - val_acc: 0.7327\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.6752 - acc: 0.7483 - val_loss: 0.6733 - val_acc: 0.7369\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.6673 - acc: 0.7520 - val_loss: 0.6655 - val_acc: 0.7362\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.6581 - acc: 0.7602 - val_loss: 0.6568 - val_acc: 0.7466\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.6480 - acc: 0.7641 - val_loss: 0.6472 - val_acc: 0.7520\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.6370 - acc: 0.7722 - val_loss: 0.6371 - val_acc: 0.7573\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.6254 - acc: 0.7755 - val_loss: 0.6263 - val_acc: 0.7618\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.6134 - acc: 0.7825 - val_loss: 0.6154 - val_acc: 0.7677\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.6011 - acc: 0.7883 - val_loss: 0.6041 - val_acc: 0.7729\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.5886 - acc: 0.7939 - val_loss: 0.5929 - val_acc: 0.7782\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.5761 - acc: 0.8009 - val_loss: 0.5816 - val_acc: 0.7842\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.5637 - acc: 0.8062 - val_loss: 0.5704 - val_acc: 0.7901\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.5512 - acc: 0.8129 - val_loss: 0.5595 - val_acc: 0.7965\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.5390 - acc: 0.8176 - val_loss: 0.5487 - val_acc: 0.8004\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.5270 - acc: 0.8236 - val_loss: 0.5378 - val_acc: 0.8055\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.5152 - acc: 0.8293 - val_loss: 0.5274 - val_acc: 0.8108\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.5036 - acc: 0.8337 - val_loss: 0.5173 - val_acc: 0.8144\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4925 - acc: 0.8379 - val_loss: 0.5074 - val_acc: 0.8195\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.4817 - acc: 0.8427 - val_loss: 0.4978 - val_acc: 0.8239\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4711 - acc: 0.8469 - val_loss: 0.4886 - val_acc: 0.8262\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.4610 - acc: 0.8518 - val_loss: 0.4798 - val_acc: 0.8318\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4512 - acc: 0.8553 - val_loss: 0.4712 - val_acc: 0.8338\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4419 - acc: 0.8591 - val_loss: 0.4630 - val_acc: 0.8358\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4328 - acc: 0.8623 - val_loss: 0.4553 - val_acc: 0.8390\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.4242 - acc: 0.8644 - val_loss: 0.4478 - val_acc: 0.8411\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.4157 - acc: 0.8681 - val_loss: 0.4408 - val_acc: 0.8443\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4078 - acc: 0.8715 - val_loss: 0.4341 - val_acc: 0.8445\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.4000 - acc: 0.8738 - val_loss: 0.4275 - val_acc: 0.8479\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.3926 - acc: 0.8759 - val_loss: 0.4214 - val_acc: 0.8497\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.3855 - acc: 0.8771 - val_loss: 0.4155 - val_acc: 0.8515\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.3787 - acc: 0.8793 - val_loss: 0.4098 - val_acc: 0.8519\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.3721 - acc: 0.8807 - val_loss: 0.4045 - val_acc: 0.8536\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.3658 - acc: 0.8831 - val_loss: 0.3994 - val_acc: 0.8548\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.3597 - acc: 0.8861 - val_loss: 0.3946 - val_acc: 0.8568\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.3539 - acc: 0.8875 - val_loss: 0.3898 - val_acc: 0.8575\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 11us/step - loss: 0.3482 - acc: 0.8894 - val_loss: 0.3854 - val_acc: 0.8592\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.3428 - acc: 0.8907 - val_loss: 0.3812 - val_acc: 0.8604\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 12us/step - loss: 0.3375 - acc: 0.8926 - val_loss: 0.3771 - val_acc: 0.8611\n",
            "25000/25000 [==============================] - 1s 36us/step\n",
            "[0.38609499324798585, 0.85608]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I4zIPJDcTPq3",
        "outputId": "c4a75b2b-9a25-468f-b48d-c41bb17e7569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "results = model2.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 34us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86d-HCd93JEP",
        "colab_type": "code",
        "outputId": "53ce55aa-8faa-4d34-aa21-5597d6aa3df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 256, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,017\n",
            "Trainable params: 160,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "waS96edDTRyL",
        "outputId": "eba2cf43-5926-4177-ce06-8043ec414d67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print (results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.38609499324798585, 0.85608]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XB7aveVzTC5a",
        "outputId": "ff17a121-a937-493a-90ff-f469ad34002f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history2.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of model2 is 87%. Using Embedding layer instead of one-hot layer improved the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "--020hfG6rN2"
      },
      "source": [
        "### Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_PypdqG9Iis",
        "colab": {}
      },
      "source": [
        "from keras.initializers import constant\n",
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gembn7VM3ex8",
        "colab": {}
      },
      "source": [
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZCPUM0W_Drc",
        "colab": {}
      },
      "source": [
        "# put the code here\n",
        "from keras.initializers import Constant\n",
        "\n",
        "wordToIndex, indexToWord, wordToGlove=readGloveFile(gloveFile='glove.6B.50d.txt')\n",
        "PretrainedEmbeddingLayer= createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable='FALSE')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "### Adding another hidden layer to the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbZ6UBDfbjea"
      },
      "source": [
        "In model3, we only add another dense layer to see if that improves the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vw0le1YjDdCa",
        "colab": {}
      },
      "source": [
        "model3 = models.Sequential()\n",
        "model3.add(PretrainedEmbeddingLayer)\n",
        "model3.add(GlobalAveragePooling1DMasked())\n",
        "model3.add(layers.Dense(16, activation='relu'))\n",
        "model3.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDpDdpsFWrt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e3bf024-eed4-4bbd-e754-660ce3a7fc62"
      },
      "source": [
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history3 = model3.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model3.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.6952 - acc: 0.5252 - val_loss: 0.6878 - val_acc: 0.5763\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.6850 - acc: 0.5815 - val_loss: 0.6818 - val_acc: 0.5947\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.6773 - acc: 0.6193 - val_loss: 0.6735 - val_acc: 0.6384\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.6665 - acc: 0.6553 - val_loss: 0.6621 - val_acc: 0.6603\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 25us/step - loss: 0.6510 - acc: 0.6814 - val_loss: 0.6449 - val_acc: 0.6853\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.6283 - acc: 0.7052 - val_loss: 0.6194 - val_acc: 0.7088\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.5958 - acc: 0.7364 - val_loss: 0.5856 - val_acc: 0.7404\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.5553 - acc: 0.7693 - val_loss: 0.5459 - val_acc: 0.7723\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.5097 - acc: 0.8029 - val_loss: 0.5040 - val_acc: 0.7947\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.4641 - acc: 0.8251 - val_loss: 0.4636 - val_acc: 0.8197\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.4211 - acc: 0.8464 - val_loss: 0.4290 - val_acc: 0.8361\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.3850 - acc: 0.8600 - val_loss: 0.4010 - val_acc: 0.8460\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.3547 - acc: 0.8714 - val_loss: 0.3792 - val_acc: 0.8504\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.3290 - acc: 0.8805 - val_loss: 0.3607 - val_acc: 0.8621\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.3078 - acc: 0.8893 - val_loss: 0.3496 - val_acc: 0.8595\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2898 - acc: 0.8951 - val_loss: 0.3365 - val_acc: 0.8684\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2729 - acc: 0.9017 - val_loss: 0.3265 - val_acc: 0.8727\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2587 - acc: 0.9071 - val_loss: 0.3199 - val_acc: 0.8717\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2458 - acc: 0.9105 - val_loss: 0.3129 - val_acc: 0.8758\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2336 - acc: 0.9159 - val_loss: 0.3082 - val_acc: 0.8778\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 25us/step - loss: 0.2227 - acc: 0.9201 - val_loss: 0.3038 - val_acc: 0.8774\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2125 - acc: 0.9245 - val_loss: 0.3003 - val_acc: 0.8790\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2031 - acc: 0.9274 - val_loss: 0.2979 - val_acc: 0.8803\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1946 - acc: 0.9321 - val_loss: 0.2957 - val_acc: 0.8814\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1859 - acc: 0.9357 - val_loss: 0.2943 - val_acc: 0.8811\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1783 - acc: 0.9387 - val_loss: 0.2934 - val_acc: 0.8810\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1714 - acc: 0.9413 - val_loss: 0.2926 - val_acc: 0.8820\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1640 - acc: 0.9448 - val_loss: 0.2927 - val_acc: 0.8819\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1576 - acc: 0.9479 - val_loss: 0.2939 - val_acc: 0.8814\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1509 - acc: 0.9516 - val_loss: 0.2933 - val_acc: 0.8828\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1453 - acc: 0.9538 - val_loss: 0.2945 - val_acc: 0.8830\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1397 - acc: 0.9574 - val_loss: 0.2985 - val_acc: 0.8818\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1348 - acc: 0.9574 - val_loss: 0.2962 - val_acc: 0.8835\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1293 - acc: 0.9599 - val_loss: 0.2986 - val_acc: 0.8828\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1249 - acc: 0.9612 - val_loss: 0.3001 - val_acc: 0.8824\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1196 - acc: 0.9642 - val_loss: 0.3020 - val_acc: 0.8813\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1168 - acc: 0.9640 - val_loss: 0.3054 - val_acc: 0.8822\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 25us/step - loss: 0.1108 - acc: 0.9681 - val_loss: 0.3071 - val_acc: 0.8812\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1068 - acc: 0.9692 - val_loss: 0.3101 - val_acc: 0.8809\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1030 - acc: 0.9711 - val_loss: 0.3137 - val_acc: 0.8814\n",
            "25000/25000 [==============================] - 1s 38us/step\n",
            "[0.3307963452911377, 0.87044]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCrPg9ncOQIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5770fc22-4719-4a14-9c4c-d1cb19ebb727"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.3307963452911377, 0.87044]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVQnJBsuOCMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "a4ca65ff-7517-4aae-c092-0f18bca191b4"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 50)          20000050  \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                816       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 20,000,883\n",
            "Trainable params: 20,000,883\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QtsdVeW7UgCu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fff639b7-a616-437c-cc1e-e40a9d352541"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history3.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEclzSgQiInnu/wNCWM9v6u7BdQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx--Ytk3ZbLo"
      },
      "source": [
        "The accuracy of model3 with an additional layer is 85%. Adding more layers can help you to extract more features. But we can do that upto a certain extent. After some point, instead of extracting features, we tend to overfit the data. Overfitting can lead to errors in some or the other form like false positives. It is not easy to choose the number of units in a hidden layer or the number of hidden layers in a neural network. For many applications, one hidden layer is enough. As a general rule, the number of units in that hidden layer is between the number of inputs and the number of outputs.\n",
        " The best way to decide on the number of units and hidden layers is to try various parameters. Train several neural networks with different numbers of hidden layers and neurons, and monitor the performance of them. You will have to experiment using a series of different architectures. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gn2GSV4ioyO2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XYC6DykEox2w",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GsCJ01StlgCx"
      },
      "source": [
        "This tutorial is substantially based on this document:\n",
        "https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
        "\n",
        "To read more about Sequential APIs you can go to: https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "The one-hot word vector layer is taken from:\n",
        "https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jL0UovfaE9GE",
        "colab": {}
      },
      "source": [
        "model4 = models.Sequential()\n",
        "model4.add(PretrainedEmbeddingLayer)\n",
        "model4.add(GlobalAveragePooling1DMasked())\n",
        "model4.add(layers.Dense(16, activation='relu'))\n",
        "model4.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHEatMLeP1-X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5bfd06e-4ef9-4727-b1e9-ef1167d0cfe1"
      },
      "source": [
        "model4.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history4 = model4.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 1s 69us/step - loss: 0.6539 - acc: 0.8169 - val_loss: 0.6300 - val_acc: 0.8441\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.5905 - acc: 0.9185 - val_loss: 0.5777 - val_acc: 0.8605\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.5242 - acc: 0.9237 - val_loss: 0.5192 - val_acc: 0.8686\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.4533 - acc: 0.9369 - val_loss: 0.4613 - val_acc: 0.8718\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.3868 - acc: 0.9395 - val_loss: 0.4107 - val_acc: 0.8733\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.3298 - acc: 0.9411 - val_loss: 0.3706 - val_acc: 0.8773\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2834 - acc: 0.9452 - val_loss: 0.3413 - val_acc: 0.8807\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.2476 - acc: 0.9481 - val_loss: 0.3213 - val_acc: 0.8795\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.2202 - acc: 0.9504 - val_loss: 0.3073 - val_acc: 0.8812\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1995 - acc: 0.9521 - val_loss: 0.2988 - val_acc: 0.8826\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1829 - acc: 0.9531 - val_loss: 0.2927 - val_acc: 0.8835\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1694 - acc: 0.9562 - val_loss: 0.2897 - val_acc: 0.8833\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1581 - acc: 0.9589 - val_loss: 0.2879 - val_acc: 0.8846\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1488 - acc: 0.9599 - val_loss: 0.2876 - val_acc: 0.8861\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1402 - acc: 0.9623 - val_loss: 0.2880 - val_acc: 0.8855\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1331 - acc: 0.9633 - val_loss: 0.2895 - val_acc: 0.8857\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1263 - acc: 0.9655 - val_loss: 0.2916 - val_acc: 0.8833\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1203 - acc: 0.9669 - val_loss: 0.2931 - val_acc: 0.8850\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.1148 - acc: 0.9687 - val_loss: 0.2971 - val_acc: 0.8838\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1095 - acc: 0.9700 - val_loss: 0.2990 - val_acc: 0.8842\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 24us/step - loss: 0.1052 - acc: 0.9709 - val_loss: 0.3022 - val_acc: 0.8839\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.1005 - acc: 0.9729 - val_loss: 0.3059 - val_acc: 0.8841\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.0970 - acc: 0.9738 - val_loss: 0.3093 - val_acc: 0.8832\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0926 - acc: 0.9753 - val_loss: 0.3132 - val_acc: 0.8832\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.0888 - acc: 0.9762 - val_loss: 0.3181 - val_acc: 0.8822\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0857 - acc: 0.9776 - val_loss: 0.3234 - val_acc: 0.8809\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0824 - acc: 0.9775 - val_loss: 0.3256 - val_acc: 0.8816\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0788 - acc: 0.9795 - val_loss: 0.3310 - val_acc: 0.8811\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.0758 - acc: 0.9806 - val_loss: 0.3351 - val_acc: 0.8800\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 26us/step - loss: 0.0728 - acc: 0.9813 - val_loss: 0.3419 - val_acc: 0.8795\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0704 - acc: 0.9823 - val_loss: 0.3462 - val_acc: 0.8787\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0682 - acc: 0.9829 - val_loss: 0.3495 - val_acc: 0.8790\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0650 - acc: 0.9840 - val_loss: 0.3541 - val_acc: 0.8778\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.0625 - acc: 0.9849 - val_loss: 0.3599 - val_acc: 0.8789\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0603 - acc: 0.9853 - val_loss: 0.3649 - val_acc: 0.8781\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0584 - acc: 0.9865 - val_loss: 0.3714 - val_acc: 0.8777\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 28us/step - loss: 0.0559 - acc: 0.9878 - val_loss: 0.3752 - val_acc: 0.8775\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0538 - acc: 0.9882 - val_loss: 0.3802 - val_acc: 0.8768\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0517 - acc: 0.9885 - val_loss: 0.3896 - val_acc: 0.8764\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 27us/step - loss: 0.0499 - acc: 0.9896 - val_loss: 0.3923 - val_acc: 0.8766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VjbgIRtQA09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7b73820c-c371-498a-fce0-416b52562fe2"
      },
      "source": [
        "results = model4.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 36us/step\n",
            "[0.4182323659706116, 0.86428]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBLJyE-HQCwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}